

## 1. k近邻算法简介

**k近邻算法（k-Nearest Neighbors, KNN）** 是一种基本且直观的监督学习算法，广泛应用于分类和回归任务。其主要思想是通过测量不同样本之间的距离进行预测。

**主要特点：**

- **懒惰学习（Lazy Learning）：** KNN在训练阶段不进行显式的模型训练，而是将所有训练数据存储起来，直到进行预测时才计算。
- **基于距离的分类：** 通过计算输入样本与训练样本之间的距离，选取距离最近的k个邻居，根据多数投票或平均值进行预测。
- **简单易实现：** 算法概念简单，易于理解和实现。

---

## 2. 算法原理

### 2.1 分类任务

对于一个待分类的样本，KNN算法通过以下步骤进行预测：

1. **选择参数k：** 确定要考虑的最近邻居数量。
2. **计算距离：** 计算待分类样本与所有训练样本之间的距离，常用的距离度量包括欧氏距离、曼哈顿距离等。
3. **选择k个最近邻：** 找出距离最近的k个训练样本。
4. **多数投票：** 根据这k个邻居所属的类别，进行投票表决，选择得票最多的类别作为预测结果。

### 2.2 回归任务

对于回归任务，KNN通过以下步骤进行预测：

1. 确定参数k。
2. 计算待预测样本与所有训练样本之间的距离。
3. 选择k个最近邻。
4. 对这k个邻居的目标变量取平均值，作为预测结果。

**距离度量常用公式：**

- **欧氏距离（Euclidean Distance）：**

  $$
  d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
  $$

- **曼哈顿距离（Manhattan Distance）：**

  $$
  d(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{n} |x_i - y_i|
  $$
